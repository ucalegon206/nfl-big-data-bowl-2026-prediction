{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedf2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Add repo root to path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "print(f'Repo root: {repo_root}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b377087",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_data import load_all_inputs, load_all_outputs\n",
    "from features import add_time_lag_features, prepare_features\n",
    "\n",
    "print('Loading training inputs and outputs...')\n",
    "X = load_all_inputs(repo_root / 'train')\n",
    "y = load_all_outputs(repo_root / 'train')\n",
    "\n",
    "print(f'Inputs: {len(X):,} rows')\n",
    "print(f'Outputs: {len(y):,} rows')\n",
    "print(f'Input columns: {X.columns.tolist()[:10]}...')\n",
    "print(f'Output columns: {y.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and outputs\n",
    "print('Merging training data...')\n",
    "merged = X.merge(y, on=['game_id','play_id','nfl_id','frame_id'], how='inner', suffixes=(None,'_target'))\n",
    "print(f'Merged rows: {len(merged):,}')\n",
    "print(f'Merged shape: {merged.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d577c1c",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-lag features (velocity, acceleration trends)\n",
    "print('Adding time-lag features...')\n",
    "merged = add_time_lag_features(merged)\n",
    "\n",
    "# Prepare engineered features (ball-relative, normalized coords, etc.)\n",
    "print('Preparing engineered features...')\n",
    "feat_df, feat_cols = prepare_features(merged)\n",
    "\n",
    "print(f'Feature columns ({len(feat_cols)}): {feat_cols}')\n",
    "print(f'Feature DataFrame shape: {feat_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data (remove NaNs)\n",
    "mask = feat_df[feat_cols].notnull().all(axis=1)\n",
    "feat_df_clean = feat_df[mask].reset_index(drop=True)\n",
    "merged_clean = merged.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(f'Rows after removing NaNs: {len(feat_df_clean):,}')\n",
    "print(f'Rows removed: {len(feat_df) - len(feat_df_clean):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b761ba7",
   "metadata": {},
   "source": [
    "## 3. Prepare Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data (sample for speed if needed)\n",
    "X_all = feat_df_clean[feat_cols].copy()\n",
    "y_x = merged_clean['x_target'].copy()\n",
    "y_y = merged_clean['y_target'].copy()\n",
    "\n",
    "# Sample if dataset is too large\n",
    "MAX_ROWS = 200_000\n",
    "if len(X_all) > MAX_ROWS:\n",
    "    print(f'Sampling {MAX_ROWS} rows for training (from {len(X_all):,})')\n",
    "    idx = np.random.RandomState(42).choice(len(X_all), size=MAX_ROWS, replace=False)\n",
    "    X_all = X_all.iloc[idx].reset_index(drop=True)\n",
    "    y_x = y_x.iloc[idx].reset_index(drop=True)\n",
    "    y_y = y_y.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, yx_train, yx_val, yy_train, yy_val = train_test_split(\n",
    "    X_all, y_x, y_y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training set: {len(X_train):,} rows')\n",
    "print(f'Validation set: {len(X_val):,} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a6c5c",
   "metadata": {},
   "source": [
    "## 4. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ea6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train x regressor\n",
    "print('Training x-coordinate regressor...')\n",
    "best_params_x = {'learning_rate': 0.2, 'max_iter': 400, 'max_depth': 5, 'max_bins': 255, 'min_samples_leaf': 100}\n",
    "mx = HistGradientBoostingRegressor(**best_params_x, random_state=42)\n",
    "mx.fit(X_train, yx_train)\n",
    "print('✓ x-regressor trained')\n",
    "\n",
    "# Train y regressor\n",
    "print('Training y-coordinate regressor...')\n",
    "best_params_y = {'learning_rate': 0.1, 'max_iter': 400, 'max_depth': 8, 'max_bins': 127, 'min_samples_leaf': 100}\n",
    "my = HistGradientBoostingRegressor(**best_params_y, random_state=42)\n",
    "my.fit(X_train, yy_train)\n",
    "print('✓ y-regressor trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b324c",
   "metadata": {},
   "source": [
    "## 5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fea342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "px = mx.predict(X_val)\n",
    "py = my.predict(X_val)\n",
    "\n",
    "rmse_x = np.sqrt(mean_squared_error(yx_val, px))\n",
    "rmse_y = np.sqrt(mean_squared_error(yy_val, py))\n",
    "combined_rmse = np.sqrt((rmse_x**2 + rmse_y**2)/2)\n",
    "\n",
    "print(f'Validation Results:')\n",
    "print(f'  RMSE x: {rmse_x:.4f}')\n",
    "print(f'  RMSE y: {rmse_y:.4f}')\n",
    "print(f'  Combined RMSE: {combined_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be536f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction sanity checks\n",
    "print('Prediction Validation:')\n",
    "print(f'  x predictions - min: {px.min():.2f}, max: {px.max():.2f}, mean: {px.mean():.2f}')\n",
    "print(f'  y predictions - min: {py.min():.2f}, max: {py.max():.2f}, mean: {py.mean():.2f}')\n",
    "print(f'  No NaNs in x: {not np.isnan(px).any()}')\n",
    "print(f'  No NaNs in y: {not np.isnan(py).any()}')\n",
    "print(f'  All finite x: {np.isfinite(px).all()}')\n",
    "print(f'  All finite y: {np.isfinite(py).all()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6901e",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6923be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and metadata\n",
    "meta = {\n",
    "    'feature_columns': feat_cols,\n",
    "    'models': {'x': mx, 'y': my},\n",
    "    'best_params': {'x': best_params_x, 'y': best_params_y},\n",
    "    'player_position_values': merged_clean['player_position'].dropna().unique().tolist()\n",
    "}\n",
    "\n",
    "model_path = repo_root / 'models' / 'best_model.pkl'\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(meta, model_path)\n",
    "\n",
    "print(f'Model saved to {model_path}')\n",
    "print(f'Model size: {model_path.stat().st_size / 1024 / 1024:.2f} MB')\n",
    "print('\\n✅ Training complete! Model ready for Kaggle submission.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
