{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# import kaggle_evaluation.nfl_inference_server\n",
    "# Use robust importer to handle missing module in runtime\n",
    "try:\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "except ModuleNotFoundError:\n",
    "    from pathlib import Path\n",
    "    root = Path('/kaggle/input')\n",
    "    comp = None\n",
    "    if root.exists():\n",
    "        for p in root.iterdir():\n",
    "            if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "                comp = p\n",
    "                break\n",
    "    candidates = []\n",
    "    if comp:\n",
    "        candidates.append(comp / 'kaggle_evaluation')\n",
    "    for p in root.iterdir() if root.exists() else []:\n",
    "        if p.is_dir():\n",
    "            candidates.append(p / 'kaggle_evaluation')\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            sys.path.insert(0, str(c.parent))\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "\n",
    "# Submission tracking info\n",
    "SUBMISSION_CREATED = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "SUBMISSION_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING ENSEMBLE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Submission Created: {SUBMISSION_CREATED}\")\n",
    "print(f\"Submission ID: {SUBMISSION_ID}\")\n",
    "\n",
    "\n",
    "def _find_ensemble_dir():\n",
    "    \"\"\"Search all attached datasets for nfl_ensemble_v* directories.\n",
    "    Returns path to the FIRST ensemble directory found that matches the pattern.\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n=== Searching for Ensemble Models ===\")\n",
    "    folders = [p.name for p in root.iterdir() if p.is_dir()]\n",
    "    print(f\"Available folders: {folders}\")\n",
    "    print(\"Looking for: nfl_ensemble_v* directories\")\n",
    "    \n",
    "    for dataset in root.iterdir():\n",
    "        if not dataset.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Skip the competition data folder\n",
    "        if 'nfl-big-data-bowl-2026-prediction' in dataset.name.lower():\n",
    "            print(f\"  Skipping competition folder: {dataset.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Checking dataset: {dataset.name}\")\n",
    "        \n",
    "        # Search for nfl_ensemble_v* directories\n",
    "        ensemble_candidates = list(dataset.glob('**/nfl_ensemble_v*'))\n",
    "        ensemble_candidates = [p for p in ensemble_candidates if p.is_dir()]\n",
    "        \n",
    "        if ensemble_candidates:\n",
    "            ensemble_dir = ensemble_candidates[0]\n",
    "            print(f\"✓ Found ensemble in: {dataset}\")\n",
    "            print(f\"  Ensemble at: {ensemble_dir}\")\n",
    "            return ensemble_dir\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_features_module(ensemble_root=None):\n",
    "    \"\"\"Search all attached datasets for features.py\n",
    "    \n",
    "    Args:\n",
    "        ensemble_root: Optional path where ensemble was found, to search there first\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSearching for features.py in {root}\")\n",
    "    \n",
    "    # First check if features.py is in the same location as the ensemble\n",
    "    if ensemble_root:\n",
    "        print(f\"  Checking ensemble location: {ensemble_root.parent.name}\")\n",
    "        for candidate in [\n",
    "            ensemble_root / 'features.py',\n",
    "            ensemble_root.parent / 'features.py',\n",
    "            ensemble_root.parent / 'for_kaggle' / 'features.py'\n",
    "        ]:\n",
    "            if candidate.exists():\n",
    "                print(f\"✓ Found features.py with ensemble at: {candidate}\")\n",
    "                return candidate\n",
    "    \n",
    "    # Search all folders for features.py\n",
    "    for dataset in root.iterdir():\n",
    "        if not dataset.is_dir():\n",
    "            continue\n",
    "        features_candidates = list(dataset.rglob('features.py'))\n",
    "        if features_candidates:\n",
    "            print(f\"✓ Found features.py in: {dataset.name}\")\n",
    "            return features_candidates[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_pandas(df):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Find ensemble and features paths\n",
    "ensemble_dir = _find_ensemble_dir()\n",
    "if not ensemble_dir:\n",
    "    raise FileNotFoundError(\n",
    "        \"❌ No valid ensemble found (nfl_ensemble_v* directory required).\\n\\n\"\n",
    "        \"To fix:\\n\"\n",
    "        \"1. Train ensemble: python scripts/train_ensemble.py\\n\"\n",
    "        \"2. Upload ensemble directory to Kaggle as a dataset\\n\"\n",
    "        \"3. Attach the dataset to this notebook\\n\"\n",
    "        \"4. Re-run the notebook\"\n",
    "    )\n",
    "\n",
    "features_path = _find_features_module(ensemble_root=ensemble_dir)\n",
    "if not features_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"\\nfeatures.py not found in any attached dataset.\\n\"\n",
    "        \"Solutions:\\n\"\n",
    "        \"  1. Upload features.py as a separate dataset and attach it, OR\\n\"\n",
    "        \"  2. Include features.py in the ensemble dataset package\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Found features at: {features_path}\")\n",
    "print(f\"\\n✓ Found ensemble at: {ensemble_dir}\")\n",
    "\n",
    "# Write paths to environment variables so predict() can find them\n",
    "# This avoids having ANY objects in global scope that could be pickled\n",
    "os.environ['NFL_ENSEMBLE_DIR'] = str(ensemble_dir)\n",
    "os.environ['NFL_FEATURES_PATH'] = str(features_path)\n",
    "\n",
    "print(f\"✓ Stored paths in environment\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inference function used by the NFL evaluation gateway.\n",
    "    \n",
    "    CRITICAL: This function is called IN THE SERVER PROCESS via gRPC.\n",
    "    We read paths from environment variables and load models fresh here.\n",
    "    Models are loaded using native formats (LightGBM .txt, XGBoost .json)\n",
    "    to completely avoid pickle serialization issues.\n",
    "    \"\"\"\n",
    "    # Read paths from environment (set in notebook process)\n",
    "    ensemble_dir_str = os.environ.get('NFL_ENSEMBLE_DIR')\n",
    "    features_path_str = os.environ.get('NFL_FEATURES_PATH')\n",
    "    \n",
    "    if not ensemble_dir_str or not features_path_str:\n",
    "        raise RuntimeError(\"Ensemble/features paths not found in environment\")\n",
    "    \n",
    "    ensemble_dir = Path(ensemble_dir_str)\n",
    "    \n",
    "    # Add features location to path\n",
    "    features_parent = str(Path(features_path_str).parent)\n",
    "    if features_parent not in sys.path:\n",
    "        sys.path.insert(0, features_parent)\n",
    "    \n",
    "    from features import add_time_lag_features, prepare_features, transform_for_inference\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_path = ensemble_dir / 'metadata.json'\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    feat_cols = metadata['feature_columns']\n",
    "    player_pos_vals = metadata.get('player_position_values', None)\n",
    "    weights = metadata['ensemble_weights']\n",
    "    \n",
    "    # Load LightGBM models (native .txt format - NO PICKLE)\n",
    "    lgb_x = lgb.Booster(model_file=str(ensemble_dir / metadata['model_files']['lgb_x']))\n",
    "    lgb_y = lgb.Booster(model_file=str(ensemble_dir / metadata['model_files']['lgb_y']))\n",
    "    \n",
    "    # Load XGBoost models (native .json format - NO PICKLE)\n",
    "    xgb_x = xgb.Booster()\n",
    "    xgb_x.load_model(str(ensemble_dir / metadata['model_files']['xgb_x']))\n",
    "    xgb_y = xgb.Booster()\n",
    "    xgb_y.load_model(str(ensemble_dir / metadata['model_files']['xgb_y']))\n",
    "\n",
    "    # Convert inputs to pandas for feature pipeline\n",
    "    test_pd = _to_pandas(test)\n",
    "    test_in_pd = _to_pandas(test_input)\n",
    "\n",
    "    # Merge like training: left join on identifiers if available\n",
    "    key_cols = [c for c in ['game_id','play_id','nfl_id','frame_id'] \n",
    "               if c in test_pd.columns and c in test_in_pd.columns]\n",
    "    if key_cols:\n",
    "        df = pd.merge(test_pd, test_in_pd, on=key_cols, how='left', suffixes=(None,'_in'))\n",
    "    else:\n",
    "        df = test_pd.copy()\n",
    "\n",
    "    # Feature engineering for inference\n",
    "    df = add_time_lag_features(df)\n",
    "    _ = prepare_features(df)\n",
    "    X_pred = transform_for_inference(df, feat_cols, player_pos_vals)\n",
    "\n",
    "    # Predict with ensemble (weighted average)\n",
    "    lgb_px = lgb_x.predict(X_pred)\n",
    "    xgb_px = xgb_x.predict(xgb.DMatrix(X_pred))\n",
    "    px = weights['lightgbm'] * lgb_px + weights['xgboost'] * xgb_px\n",
    "    \n",
    "    lgb_py = lgb_y.predict(X_pred)\n",
    "    xgb_py = xgb_y.predict(xgb.DMatrix(X_pred))\n",
    "    py = weights['lightgbm'] * lgb_py + weights['xgboost'] * xgb_py\n",
    "\n",
    "    predictions = pd.DataFrame({'x': px, 'y': py})\n",
    "    assert len(predictions) == len(test_pd)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Start inference server (serve on hidden test; local gateway otherwise)\n",
    "inference_server = nfl_inf.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Provide path to published public competition files for local gateway\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
