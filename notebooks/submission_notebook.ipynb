{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# import kaggle_evaluation.nfl_inference_server\n",
    "# Use robust importer to handle missing module in runtime\n",
    "try:\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "except ModuleNotFoundError:\n",
    "    from pathlib import Path\n",
    "    root = Path('/kaggle/input')\n",
    "    comp = None\n",
    "    if root.exists():\n",
    "        for p in root.iterdir():\n",
    "            if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "                comp = p\n",
    "                break\n",
    "    candidates = []\n",
    "    if comp:\n",
    "        candidates.append(comp / 'kaggle_evaluation')\n",
    "    for p in root.iterdir() if root.exists() else []:\n",
    "        if p.is_dir():\n",
    "            candidates.append(p / 'kaggle_evaluation')\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            sys.path.insert(0, str(c.parent))\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "\n",
    "# Submission tracking info\n",
    "SUBMISSION_CREATED = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "SUBMISSION_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING MODEL AND FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Submission Created: {SUBMISSION_CREATED}\")\n",
    "print(f\"Submission ID: {SUBMISSION_ID}\")\n",
    "\n",
    "\n",
    "def _find_model():\n",
    "    \"\"\"Search all attached datasets for nfl_model_v*.pkl files.\n",
    "    Returns path to the FIRST model found that matches the pattern.\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n=== Searching for Model ===\")\n",
    "    folders = [p.name for p in root.iterdir() if p.is_dir()]\n",
    "    print(f\"Available folders: {folders}\")\n",
    "    print(\"POLICY: Only accepting nfl_model_v*.pkl (NEW pattern)\")\n",
    "    print(\"        Rejecting best_model_*.pkl (OLD pattern with NumPy issues)\")\n",
    "    \n",
    "    for dataset in root.iterdir():\n",
    "        if not dataset.is_dir():\n",
    "            continue\n",
    "        \n",
    "        # Skip the competition data folder\n",
    "        if 'nfl-big-data-bowl-2026-prediction' in dataset.name.lower():\n",
    "            print(f\"  Skipping competition folder: {dataset.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Checking dataset: {dataset.name}\")\n",
    "        \n",
    "        # Search for nfl_model_v*.pkl files\n",
    "        model_candidates = list(dataset.rglob('nfl_model_v*.pkl'))\n",
    "        \n",
    "        if model_candidates:\n",
    "            model_path = model_candidates[0]\n",
    "            print(f\"✓ Found model in: {dataset}\")\n",
    "            print(f\"  Model at: {model_path}\")\n",
    "            return model_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_features_module(model_root=None):\n",
    "    \"\"\"Search all attached datasets for features.py\n",
    "    \n",
    "    Args:\n",
    "        model_root: Optional path where model was found, to search there first\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSearching for features.py in {root}\")\n",
    "    \n",
    "    # First check if features.py is in the same location as the model\n",
    "    if model_root:\n",
    "        print(f\"  Checking model location: {model_root.name}\")\n",
    "        for candidate in [\n",
    "            model_root / 'features.py',\n",
    "            model_root / 'scikitlearn' / 'default' / '1' / 'features.py',\n",
    "            model_root / 'for_kaggle' / 'features.py'\n",
    "        ]:\n",
    "            if candidate.exists():\n",
    "                print(f\"✓ Found features.py with model at: {candidate}\")\n",
    "                return candidate\n",
    "    \n",
    "    # Search all folders for features.py\n",
    "    for dataset in root.iterdir():\n",
    "        if not dataset.is_dir():\n",
    "            continue\n",
    "        features_candidates = list(dataset.rglob('features.py'))\n",
    "        if features_candidates:\n",
    "            print(f\"✓ Found features.py in: {dataset.name}\")\n",
    "            return features_candidates[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def _to_pandas(df):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Global variables to store paths (NOT the model objects)\n",
    "# These will be pickled and sent to server process\n",
    "_MODEL_PATH = None\n",
    "_FEATURES_PATH = None\n",
    "\n",
    "# Find model and features paths (but don't load yet)\n",
    "_MODEL_PATH = _find_model()\n",
    "if not _MODEL_PATH:\n",
    "    raise FileNotFoundError(\n",
    "        \"❌ No valid model found (nfl_model_v*.pkl pattern required).\\n\"\n",
    "        \"The old best_model_*.pkl files are rejected due to NumPy PCG64 compatibility issues.\\n\\n\"\n",
    "        \"To fix:\\n\"\n",
    "        \"1. Delete old 'nfl-model-v*' datasets from Kaggle (without time component)\\n\"\n",
    "        \"2. Re-upload using timestamped dataset name: nfl-model-v{YYYYMMDD-HHMMSS}\\n\"\n",
    "        \"3. Ensure for_kaggle.zip contains nfl_model_v*.pkl (not best_model_*.pkl)\\n\"\n",
    "        \"4. Re-run the notebook\"\n",
    "    )\n",
    "\n",
    "_FEATURES_PATH = _find_features_module(model_root=_MODEL_PATH.parent.parent)\n",
    "if not _FEATURES_PATH:\n",
    "    raise FileNotFoundError(\n",
    "        \"\\nfeatures.py not found in any attached dataset.\\n\"\n",
    "        \"Solutions:\\n\"\n",
    "        \"  1. Upload features.py as a separate dataset and attach it, OR\\n\"\n",
    "        \"  2. Upload for_kaggle.zip as a dataset (not Model Registry) - it contains both model and features.py\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✓ Loading features from: {_FEATURES_PATH}\")\n",
    "print(f\"✓ Loading model from: {_MODEL_PATH}\")\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Inference function used by the NFL evaluation gateway.\n",
    "    \n",
    "    CRITICAL: This function is called IN THE SERVER PROCESS via gRPC.\n",
    "    We must load the model fresh here, not use pre-loaded objects from the notebook.\n",
    "    \"\"\"\n",
    "    # Load model fresh in THIS process (server process, not notebook process)\n",
    "    # This avoids PCG64 serialization issues\n",
    "    global _MODEL_PATH, _FEATURES_PATH\n",
    "    \n",
    "    # Add features location to path\n",
    "    sys.path.insert(0, str(_FEATURES_PATH.parent))\n",
    "    from features import add_time_lag_features, prepare_features, transform_for_inference\n",
    "    \n",
    "    # Load model in this process\n",
    "    meta = joblib.load(str(_MODEL_PATH))\n",
    "    mx = meta['models']['x']\n",
    "    my = meta['models']['y']\n",
    "    feat_cols = meta['feature_columns']\n",
    "    player_pos_vals = meta.get('player_position_values', None)\n",
    "\n",
    "    # Convert inputs to pandas for feature pipeline\n",
    "    test_pd = _to_pandas(test)\n",
    "    test_in_pd = _to_pandas(test_input)\n",
    "\n",
    "    # Merge like training: left join on identifiers if available\n",
    "    key_cols = [c for c in ['game_id','play_id','nfl_id','frame_id'] \n",
    "               if c in test_pd.columns and c in test_in_pd.columns]\n",
    "    if key_cols:\n",
    "        df = pd.merge(test_pd, test_in_pd, on=key_cols, how='left', suffixes=(None,'_in'))\n",
    "    else:\n",
    "        df = test_pd.copy()\n",
    "\n",
    "    # Feature engineering for inference\n",
    "    df = add_time_lag_features(df)\n",
    "    _ = prepare_features(df)\n",
    "    X_pred = transform_for_inference(df, feat_cols, player_pos_vals)\n",
    "\n",
    "    # Predict\n",
    "    px = mx.predict(X_pred)\n",
    "    py = my.predict(X_pred)\n",
    "\n",
    "    predictions = pd.DataFrame({'x': px, 'y': py})\n",
    "    assert len(predictions) == len(test_pd)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Start inference server (serve on hidden test; local gateway otherwise)\n",
    "inference_server = nfl_inf.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Provide path to published public competition files for local gateway\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
