{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# import kaggle_evaluation.nfl_inference_server\n",
    "# Use robust importer to handle missing module in runtime\n",
    "try:\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "except ModuleNotFoundError:\n",
    "    from pathlib import Path\n",
    "    root = Path('/kaggle/input')\n",
    "    comp = None\n",
    "    if root.exists():\n",
    "        for p in root.iterdir():\n",
    "            if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "                comp = p\n",
    "                break\n",
    "    candidates = []\n",
    "    if comp:\n",
    "        candidates.append(comp / 'kaggle_evaluation')\n",
    "    for p in root.iterdir() if root.exists() else []:\n",
    "        if p.is_dir():\n",
    "            candidates.append(p / 'kaggle_evaluation')\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            sys.path.insert(0, str(c.parent))\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "\n",
    "# Global cache to load model/features once on first call\n",
    "_MODEL_CACHE = {\n",
    "    'loaded': False,\n",
    "    'mx': None,\n",
    "    'my': None,\n",
    "    'feat_cols': None,\n",
    "    'player_pos_vals': None,\n",
    "    'attached_root': None,\n",
    "    'model_path': None\n",
    "}\n",
    "\n",
    "\n",
    "def _find_features_module(model_root=None):\n",
    "    \"\"\"Search all attached datasets for features.py\n",
    "    \n",
    "    Args:\n",
    "        model_root: Optional path where model was found, to search there first\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSearching for features.py in {root}\")\n",
    "    \n",
    "    # First check if features.py is in the same location as the model\n",
    "    if model_root:\n",
    "        print(f\"  Checking model location: {model_root.name}\")\n",
    "        for candidate in [\n",
    "            model_root / 'features.py',\n",
    "            model_root / 'scikitlearn' / 'default' / '1' / 'features.py',\n",
    "            model_root / 'for_kaggle' / 'features.py'\n",
    "        ]:\n",
    "            if candidate.exists():\n",
    "                print(f\"✓ Found features.py with model at: {candidate}\")\n",
    "                return candidate\n",
    "    \n",
    "    # Search all folders for features.py\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' not in p.name:\n",
    "            if p == model_root:\n",
    "                continue  # Already checked above\n",
    "            print(f\"  Checking: {p.name}\")\n",
    "            # Check root level\n",
    "            if (p / 'features.py').exists():\n",
    "                print(f\"✓ Found features.py in: {p}\")\n",
    "                return p / 'features.py'\n",
    "            # Check nested locations\n",
    "            for nested in p.rglob('features.py'):\n",
    "                print(f\"✓ Found features.py at: {nested}\")\n",
    "                return nested\n",
    "    \n",
    "    print(\"⚠️  No features.py found in any dataset\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_model_file():\n",
    "    \"\"\"Search all attached datasets for best_model.pkl\"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"⚠️  /kaggle/input does not exist\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n=== Searching for Model ===\")\n",
    "    print(f\"Available folders: {[p.name for p in root.iterdir() if p.is_dir()]}\")\n",
    "    \n",
    "    candidates = sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "    \n",
    "    for p in candidates:\n",
    "        # Skip the competition data folder\n",
    "        if 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "            print(f\"  Skipping competition folder: {p.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Checking dataset: {p.name}\")\n",
    "        \n",
    "        # Check for Kaggle Model Registry format: scikitlearn/default/1/best_model.pkl\n",
    "        model_registry_path = p / 'scikitlearn' / 'default' / '1' / 'best_model.pkl'\n",
    "        if model_registry_path.exists():\n",
    "            print(f\"✓ Found Kaggle model registry: {p}\")\n",
    "            print(f\"  Model at: {model_registry_path}\")\n",
    "            return p, model_registry_path\n",
    "        \n",
    "        # Check for dataset format: models/best_model.pkl\n",
    "        dataset_model_path = p / 'models' / 'best_model.pkl'\n",
    "        if dataset_model_path.exists():\n",
    "            print(f\"✓ Found model dataset: {p}\")\n",
    "            print(f\"  Model at: {dataset_model_path}\")\n",
    "            return p, dataset_model_path\n",
    "        \n",
    "        # Check for nested for_kaggle structure\n",
    "        nested_model_path = p / 'for_kaggle' / 'models' / 'best_model.pkl'\n",
    "        if nested_model_path.exists():\n",
    "            print(f\"✓ Found model in for_kaggle subfolder: {p}\")\n",
    "            print(f\"  Model at: {nested_model_path}\")\n",
    "            return p, nested_model_path\n",
    "        \n",
    "        # Search for any best_model.pkl\n",
    "        pkl_files = list(p.glob('**/best_model.pkl'))\n",
    "        if pkl_files:\n",
    "            print(f\"✓ Found model in: {p}\")\n",
    "            print(f\"  Model at: {pkl_files[0]}\")\n",
    "            return p, pkl_files[0]\n",
    "        \n",
    "        print(f\"  No model found in {p.name}\")\n",
    "    \n",
    "    print(\"\\n⚠️  No model found in any attached dataset\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _lazy_load_model_and_modules():\n",
    "    if _MODEL_CACHE['loaded']:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"LOADING MODEL AND FEATURES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find model file\n",
    "    attached, model_path = _find_model_file()\n",
    "    if not attached or not model_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"No attached model found. Please attach a dataset containing best_model.pkl. \"\n",
    "            \"Make sure the dataset is attached in the notebook's 'Input' section.\"\n",
    "        )\n",
    "    \n",
    "    _MODEL_CACHE['attached_root'] = attached\n",
    "    _MODEL_CACHE['model_path'] = model_path\n",
    "    \n",
    "    # Find features.py (may be in different dataset than model, or with the model)\n",
    "    features_path = _find_features_module(model_root=attached)\n",
    "    if not features_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"\\nfeatures.py not found in any attached dataset.\\n\"\n",
    "            \"Solutions:\\n\"\n",
    "            \"  1. Upload features.py as a separate dataset and attach it, OR\\n\"\n",
    "            \"  2. Upload for_kaggle.zip as a dataset (not Model Registry) - it contains both model and features.py\"\n",
    "        )\n",
    "    \n",
    "    # Add features location to path\n",
    "    sys.path.insert(0, str(features_path.parent))\n",
    "    print(f\"\\n✓ Loading features from: {features_path}\")\n",
    "    \n",
    "    from features import add_time_lag_features, prepare_features, transform_for_inference  # noqa: F401\n",
    "    _MODEL_CACHE['add_time_lag_features'] = add_time_lag_features\n",
    "    _MODEL_CACHE['prepare_features'] = prepare_features\n",
    "    _MODEL_CACHE['transform_for_inference'] = transform_for_inference\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"✓ Loading model from: {model_path}\")\n",
    "    meta = joblib.load(model_path)\n",
    "    _MODEL_CACHE['mx'] = meta['models']['x']\n",
    "    _MODEL_CACHE['my'] = meta['models']['y']\n",
    "    _MODEL_CACHE['feat_cols'] = meta['feature_columns']\n",
    "    _MODEL_CACHE['player_pos_vals'] = meta.get('player_position_values', None)\n",
    "    _MODEL_CACHE['loaded'] = True\n",
    "    print(f\"✓ Model loaded successfully with {len(_MODEL_CACHE['feat_cols'])} features\")\n",
    "    print(f\"✓ X model random_state: {_MODEL_CACHE['mx'].random_state}\")\n",
    "    print(f\"✓ Y model random_state: {_MODEL_CACHE['my'].random_state}\")\n",
    "    print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "def _to_pandas(df):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Inference function used by the NFL evaluation gateway.\n",
    "    Loads model + feature funcs on first call, then predicts x,y for incoming batch.\n",
    "    Returns either Polars or Pandas DataFrame with columns ['x','y'] of equal length to `test`.\n",
    "    \"\"\"\n",
    "    _lazy_load_model_and_modules()\n",
    "    add_time_lag_features = _MODEL_CACHE['add_time_lag_features']\n",
    "    prepare_features = _MODEL_CACHE['prepare_features']\n",
    "    transform_for_inference = _MODEL_CACHE['transform_for_inference']\n",
    "    mx = _MODEL_CACHE['mx']\n",
    "    my = _MODEL_CACHE['my']\n",
    "    feat_cols = _MODEL_CACHE['feat_cols']\n",
    "    player_pos_vals = _MODEL_CACHE['player_pos_vals']\n",
    "\n",
    "    # Convert inputs to pandas for feature pipeline\n",
    "    test_pd = _to_pandas(test)\n",
    "    test_in_pd = _to_pandas(test_input)\n",
    "\n",
    "    # Merge like training: left join on identifiers if available\n",
    "    key_cols = [c for c in ['game_id','play_id','nfl_id','frame_id'] if c in test_pd.columns and c in test_in_pd.columns]\n",
    "    if key_cols:\n",
    "        df = pd.merge(test_pd, test_in_pd, on=key_cols, how='left', suffixes=(None,'_in'))\n",
    "    else:\n",
    "        df = test_pd.copy()\n",
    "\n",
    "    # Feature engineering for inference\n",
    "    df = add_time_lag_features(df)\n",
    "    _ = prepare_features(df)\n",
    "    X_pred = transform_for_inference(df, feat_cols, player_pos_vals)\n",
    "\n",
    "    # Predict\n",
    "    px = mx.predict(X_pred)\n",
    "    py = my.predict(X_pred)\n",
    "\n",
    "    predictions = pd.DataFrame({'x': px, 'y': py})\n",
    "    assert len(predictions) == len(test_pd)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Start inference server (serve on hidden test; local gateway otherwise)\n",
    "inference_server = nfl_inf.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Provide path to published public competition files for local gateway\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
