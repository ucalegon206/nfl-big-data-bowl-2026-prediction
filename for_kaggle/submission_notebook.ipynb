{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# import kaggle_evaluation.nfl_inference_server\n",
    "# Use robust importer to handle missing module in runtime\n",
    "try:\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "except ModuleNotFoundError:\n",
    "    from pathlib import Path\n",
    "    root = Path('/kaggle/input')\n",
    "    comp = None\n",
    "    if root.exists():\n",
    "        for p in root.iterdir():\n",
    "            if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "                comp = p\n",
    "                break\n",
    "    candidates = []\n",
    "    if comp:\n",
    "        candidates.append(comp / 'kaggle_evaluation')\n",
    "    for p in root.iterdir() if root.exists() else []:\n",
    "        if p.is_dir():\n",
    "            candidates.append(p / 'kaggle_evaluation')\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            sys.path.insert(0, str(c.parent))\n",
    "    import kaggle_evaluation.nfl_inference_server as nfl_inf\n",
    "\n",
    "# Submission tracking info\n",
    "SUBMISSION_CREATED = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "SUBMISSION_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Global cache to load model/features once on first call\n",
    "_MODEL_CACHE = {\n",
    "    'loaded': False,\n",
    "    'mx': None,\n",
    "    'my': None,\n",
    "    'feat_cols': None,\n",
    "    'player_pos_vals': None,\n",
    "    'attached_root': None,\n",
    "    'model_path': None,\n",
    "    'model_version': None,\n",
    "    'submission_created': SUBMISSION_CREATED,\n",
    "    'submission_id': SUBMISSION_ID\n",
    "}\n",
    "\n",
    "\n",
    "def _find_features_module(model_root=None):\n",
    "    \"\"\"Search all attached datasets for features.py\n",
    "    \n",
    "    Args:\n",
    "        model_root: Optional path where model was found, to search there first\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"âš ï¸  /kaggle/input does not exist\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nSearching for features.py in {root}\")\n",
    "    \n",
    "    # First check if features.py is in the same location as the model\n",
    "    if model_root:\n",
    "        print(f\"  Checking model location: {model_root.name}\")\n",
    "        for candidate in [\n",
    "            model_root / 'features.py',\n",
    "            model_root / 'scikitlearn' / 'default' / '1' / 'features.py',\n",
    "            model_root / 'for_kaggle' / 'features.py'\n",
    "        ]:\n",
    "            if candidate.exists():\n",
    "                print(f\"âœ“ Found features.py with model at: {candidate}\")\n",
    "                return candidate\n",
    "    \n",
    "    # Search all folders for features.py\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir() and 'nfl-big-data-bowl-2026-prediction' not in p.name:\n",
    "            if p == model_root:\n",
    "                continue  # Already checked above\n",
    "            print(f\"  Checking: {p.name}\")\n",
    "            # Check root level\n",
    "            if (p / 'features.py').exists():\n",
    "                print(f\"âœ“ Found features.py in: {p}\")\n",
    "                return p / 'features.py'\n",
    "            # Check nested locations\n",
    "            for nested in p.rglob('features.py'):\n",
    "                print(f\"âœ“ Found features.py at: {nested}\")\n",
    "                return nested\n",
    "    \n",
    "    print(\"âš ï¸  No features.py found in any dataset\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _find_model_file():\n",
    "    \"\"\"Search all attached datasets for model files\n",
    "    \n",
    "    Priority order:\n",
    "    1. NEW pattern ONLY: nfl_model_v{timestamp}.pkl - newest first (defeats Kaggle caching!)\n",
    "       - OLD pattern (best_model_*.pkl) is REJECTED to avoid NumPy compatibility issues\n",
    "    \"\"\"\n",
    "    root = Path('/kaggle/input')\n",
    "    if not root.exists():\n",
    "        print(\"âš ï¸  /kaggle/input does not exist\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\n=== Searching for Model ===\")\n",
    "    print(f\"Available folders: {[p.name for p in root.iterdir() if p.is_dir()]}\")\n",
    "    print(f\"POLICY: Only accepting nfl_model_v*.pkl (NEW pattern)\")\n",
    "    print(f\"        Rejecting best_model_*.pkl (OLD pattern with NumPy issues)\")\n",
    "    \n",
    "    candidates = sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
    "    \n",
    "    for p in candidates:\n",
    "        # Skip the competition data folder\n",
    "        if 'nfl-big-data-bowl-2026-prediction' in p.name:\n",
    "            print(f\"  Skipping competition folder: {p.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Checking dataset: {p.name}\")\n",
    "        \n",
    "        # Search for all .pkl files in standard locations\n",
    "        search_patterns = [\n",
    "            p / 'scikitlearn' / 'default' / '1',  # Model Registry\n",
    "            p / 'models',  # Dataset format\n",
    "            p / 'for_kaggle' / 'models',  # Nested format\n",
    "        ]\n",
    "        \n",
    "        for search_dir in search_patterns:\n",
    "            if not search_dir.exists():\n",
    "                continue\n",
    "            \n",
    "            # Find model files - ONLY accept NEW pattern\n",
    "            model_files = []\n",
    "            \n",
    "            # NEW pattern ONLY: nfl_model_v{timestamp}.pkl (to defeat Kaggle caching)\n",
    "            new_pattern = sorted(search_dir.glob('nfl_model_v[0-9]*.pkl'), reverse=True)\n",
    "            model_files.extend(new_pattern)\n",
    "            \n",
    "            # REJECT OLD pattern: best_model_*.pkl (causes NumPy compatibility issues)\n",
    "            old_pattern = sorted(search_dir.glob('best_model_[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]_[0-9][0-9][0-9][0-9][0-9][0-9].pkl'), reverse=True)\n",
    "            if old_pattern:\n",
    "                print(f\"    âš ï¸  WARNING: Found OLD pattern models, REJECTING them:\")\n",
    "                for old_file in old_pattern:\n",
    "                    print(f\"      - {old_file.name} (causes PCG64 NumPy error)\")\n",
    "            \n",
    "            # Fallback: REJECT non-versioned best_model.pkl\n",
    "            if (search_dir / 'best_model.pkl').exists():\n",
    "                print(f\"    âš ï¸  WARNING: Found best_model.pkl (non-versioned), REJECTING it\")\n",
    "            \n",
    "            if model_files:\n",
    "                chosen = model_files[0]\n",
    "                print(f\"âœ“ Found model in: {p}\")\n",
    "                print(f\"  Model at: {chosen}\")\n",
    "                if len(model_files) > 1:\n",
    "                    print(f\"  (Found {len(model_files)} models, using newest: {chosen.name})\")\n",
    "                print(f\"  ðŸŽ¯ Using NEW naming pattern (defeats caching, avoids NumPy issues)\")\n",
    "                return p, chosen\n",
    "        \n",
    "        # Fallback: search for any NEW pattern model recursively, REJECT old patterns\n",
    "        all_new = list(p.glob('**/nfl_model_v*.pkl'))\n",
    "        if all_new:\n",
    "            chosen = sorted(all_new, reverse=True)[0]\n",
    "            print(f\"âœ“ Found model in: {p}\")\n",
    "            print(f\"  Model at: {chosen}\")\n",
    "            return p, chosen\n",
    "        \n",
    "        # Check for old pattern and warn\n",
    "        all_old = list(p.glob('**/best_model*.pkl'))\n",
    "        if all_old:\n",
    "            print(f\"  âš ï¸  Found OLD pattern models in {p.name} but REJECTING them:\")\n",
    "            for old_file in sorted(all_old, reverse=True)[:3]:\n",
    "                print(f\"    - {old_file.name}\")\n",
    "            print(f\"     (These cause PCG64 BitGenerator NumPy compatibility errors)\")\n",
    "        \n",
    "        print(f\"  No valid NEW pattern model found in {p.name}\")\n",
    "    \n",
    "    print(\"\\nâŒ ERROR: No valid model found (nfl_model_v*.pkl pattern required)\")\n",
    "    print(\"   Available but REJECTED old pattern models cause PCG64 NumPy errors\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def _lazy_load_model_and_modules():\n",
    "    if _MODEL_CACHE['loaded']:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LOADING MODEL AND FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Submission Created: {_MODEL_CACHE['submission_created']}\")\n",
    "    print(f\"Submission ID: {_MODEL_CACHE['submission_id']}\")\n",
    "    \n",
    "    # Find model file\n",
    "    attached, model_path = _find_model_file()\n",
    "    if not attached or not model_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"âŒ No valid model found (nfl_model_v*.pkl pattern required).\\n\"\n",
    "            \"The old best_model_*.pkl files are rejected due to NumPy PCG64 compatibility issues.\\n\\n\"\n",
    "            \"To fix:\\n\"\n",
    "            \"1. Delete old 'nfl-model-v*' datasets from Kaggle (without time component)\\n\"\n",
    "            \"2. Re-upload using timestamped dataset name: nfl-model-v{YYYYMMDD-HHMMSS}\\n\"\n",
    "            \"3. Ensure for_kaggle.zip contains nfl_model_v*.pkl (not best_model_*.pkl)\\n\"\n",
    "            \"4. Re-run the notebook\"\n",
    "        )\n",
    "    \n",
    "    _MODEL_CACHE['attached_root'] = attached\n",
    "    _MODEL_CACHE['model_path'] = model_path\n",
    "    \n",
    "    # Find features.py (may be in different dataset than model, or with the model)\n",
    "    features_path = _find_features_module(model_root=attached)\n",
    "    if not features_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"\\nfeatures.py not found in any attached dataset.\\n\"\n",
    "            \"Solutions:\\n\"\n",
    "            \"  1. Upload features.py as a separate dataset and attach it, OR\\n\"\n",
    "            \"  2. Upload for_kaggle.zip as a dataset (not Model Registry) - it contains both model and features.py\"\n",
    "        )\n",
    "    \n",
    "    # Add features location to path\n",
    "    sys.path.insert(0, str(features_path.parent))\n",
    "    print(f\"\\nâœ“ Loading features from: {features_path}\")\n",
    "    \n",
    "    from features import add_time_lag_features, prepare_features, transform_for_inference  # noqa: F401\n",
    "    _MODEL_CACHE['add_time_lag_features'] = add_time_lag_features\n",
    "    _MODEL_CACHE['prepare_features'] = prepare_features\n",
    "    _MODEL_CACHE['transform_for_inference'] = transform_for_inference\n",
    "    \n",
    "    # Store model path instead of loading model objects\n",
    "    # This avoids gRPC serialization issues with sklearn models\n",
    "    print(f\"âœ“ Found model at: {model_path}\")\n",
    "    _MODEL_CACHE['model_path_str'] = str(model_path)\n",
    "    \n",
    "    # Load just to verify and print info\n",
    "    meta = joblib.load(model_path)\n",
    "    _MODEL_CACHE['feat_cols'] = meta['feature_columns']\n",
    "    _MODEL_CACHE['player_pos_vals'] = meta.get('player_position_values', None)\n",
    "    _MODEL_CACHE['loaded'] = True\n",
    "    print(f\"âœ“ Model verified with {len(_MODEL_CACHE['feat_cols'])} features\")\n",
    "    print(f\"âœ“ X model random_state: {meta['models']['x'].random_state}\")\n",
    "    print(f\"âœ“ Y model random_state: {meta['models']['y'].random_state}\")\n",
    "    \n",
    "    # Extract model version from filename if available\n",
    "    model_filename = model_path.name\n",
    "    if 'nfl_model_v' in model_filename:\n",
    "        version_id = model_filename.replace('nfl_model_v', '').replace('.pkl', '')\n",
    "        _MODEL_CACHE['model_version'] = version_id\n",
    "        print(f\"âœ“ Model Version: {version_id}\")\n",
    "    \n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "def _to_pandas(df):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "class NFLPredictor:\n",
    "    \"\"\"\n",
    "    Callable class that loads model once on initialization.\n",
    "    This avoids gRPC serialization issues - the model is loaded in the server process,\n",
    "    not passed through the gRPC boundary.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Load model and features once during initialization.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"INITIALIZING PREDICTOR IN SERVER PROCESS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Find and load model\n",
    "        model_path = _find_model()\n",
    "        if not model_path:\n",
    "            raise FileNotFoundError(\"No valid model found\")\n",
    "        \n",
    "        # Find and load features module\n",
    "        features_path = _find_features_module(model_root=model_path.parent.parent)\n",
    "        if not features_path:\n",
    "            raise FileNotFoundError(\"features.py not found\")\n",
    "        \n",
    "        sys.path.insert(0, str(features_path.parent))\n",
    "        from features import add_time_lag_features, prepare_features, transform_for_inference\n",
    "        \n",
    "        print(f\"âœ“ Loading model from: {model_path}\")\n",
    "        meta = joblib.load(model_path)\n",
    "        \n",
    "        # Store as instance variables\n",
    "        self.mx = meta['models']['x']\n",
    "        self.my = meta['models']['y']\n",
    "        self.feat_cols = meta['feature_columns']\n",
    "        self.player_pos_vals = meta.get('player_position_values', None)\n",
    "        self.add_time_lag_features = add_time_lag_features\n",
    "        self.prepare_features = prepare_features\n",
    "        self.transform_for_inference = transform_for_inference\n",
    "        \n",
    "        print(f\"âœ“ Model loaded successfully with {len(self.feat_cols)} features\")\n",
    "        print(f\"âœ“ X model random_state: {self.mx.random_state}\")\n",
    "        print(f\"âœ“ Y model random_state: {self.my.random_state}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    def __call__(self, test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict function called by the NFL evaluation gateway.\n",
    "        Returns Polars or Pandas DataFrame with columns ['x','y'].\n",
    "        \"\"\"\n",
    "        # Convert inputs to pandas for feature pipeline\n",
    "        test_pd = _to_pandas(test)\n",
    "        test_in_pd = _to_pandas(test_input)\n",
    "\n",
    "        # Merge like training: left join on identifiers if available\n",
    "        key_cols = [c for c in ['game_id','play_id','nfl_id','frame_id'] \n",
    "                   if c in test_pd.columns and c in test_in_pd.columns]\n",
    "        if key_cols:\n",
    "            df = pd.merge(test_pd, test_in_pd, on=key_cols, how='left', suffixes=(None,'_in'))\n",
    "        else:\n",
    "            df = test_pd.copy()\n",
    "\n",
    "        # Feature engineering for inference\n",
    "        df = self.add_time_lag_features(df)\n",
    "        _ = self.prepare_features(df)\n",
    "        X_pred = self.transform_for_inference(df, self.feat_cols, self.player_pos_vals)\n",
    "\n",
    "        # Predict\n",
    "        px = self.mx.predict(X_pred)\n",
    "        py = self.my.predict(X_pred)\n",
    "\n",
    "        predictions = pd.DataFrame({'x': px, 'y': py})\n",
    "        assert len(predictions) == len(test_pd)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Create predictor instance - will be initialized once when server starts\n",
    "predictor = NFLPredictor()\n",
    "\n",
    "# Start inference server (serve on hidden test; local gateway otherwise)\n",
    "inference_server = nfl_inf.NFLInferenceServer(predictor)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Provide path to published public competition files for local gateway\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
